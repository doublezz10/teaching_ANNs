{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_to_ANNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwl2PktPqQdUlJzjomnULR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doublezz10/teaching_ANNs/blob/main/intro_to_ANNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju5JRcJc-KDJ"
      },
      "source": [
        "# Toward an intuitve understanding of neural networks\n",
        "\n",
        "Assembled by Zach Zeisler\n",
        "\n",
        "Most of the content comes from YouTube (3Blue1Brown) and Neuromatch Academy\n",
        "\n",
        "None of the images or code are mine, but most of the text is unless specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs0CdHoq-VHZ"
      },
      "source": [
        "## Goals\n",
        "\n",
        "\n",
        "* Get some high-level understanding of the math that neural networks do without getting *lost in the weeds* of linear algebra\n",
        "* Understand the roles of different layers, numbers of neurons, activation functions, and learning algorithms\n",
        "* Explain backpropagation as the algorithm by which neural networks \"learn\"\n",
        "* Build a simple multi-layer perceptron to decode visual stimuli from neural activity data\n",
        "* Compare this simple network to some of its more complicated relatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWoAGdyA_onO"
      },
      "source": [
        "## Get set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IREFsIMw-Hla",
        "cellView": "form"
      },
      "source": [
        "#@title Import data, helper functions, etc.\n",
        "#@markdown This will take a second :)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "mpl.rcParams['figure.figsize'] = (12, 9)\n",
        "\n",
        "fname = \"W3D4_stringer_oribinned1.npz\"\n",
        "url = \"https://osf.io/683xc/download\"\n",
        "expected_md5 = \"436599dfd8ebe6019f066c38aed20580\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
        "      print(\"!!! Data download appears corrupted !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "\n",
        "def load_data(data_name=fname, bin_width=1):\n",
        "  \"\"\"Load mouse V1 data from Stringer et al. (2019)\n",
        "\n",
        "  Data from study reported in this preprint:\n",
        "  https://www.biorxiv.org/content/10.1101/679324v2.abstract\n",
        "\n",
        "  These data comprise time-averaged responses of ~20,000 neurons\n",
        "  to ~4,000 stimulus gratings of different orientations, recorded\n",
        "  through Calcium imaginge. The responses have been normalized by\n",
        "  spontanous levels of activity and then z-scored over stimuli, so\n",
        "  expect negative numbers. They have also been binned and averaged\n",
        "  to each degree of orientation.\n",
        "\n",
        "  This function returns the relevant data (neural responses and\n",
        "  stimulus orientations) in a torch.Tensor of data type torch.float32\n",
        "  in order to match the default data type for nn.Parameters in\n",
        "  Google Colab.\n",
        "\n",
        "  This function will actually average responses to stimuli with orientations\n",
        "  falling within bins specified by the bin_width argument. This helps\n",
        "  produce individual neural \"responses\" with smoother and more\n",
        "  interpretable tuning curves.\n",
        "\n",
        "  Args:\n",
        "    bin_width (float): size of stimulus bins over which to average neural\n",
        "      responses\n",
        "\n",
        "  Returns:\n",
        "    resp (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,\n",
        "        each row contains the responses of each neuron to a given stimulus.\n",
        "        As mentioned above, neural \"response\" is actually an average over\n",
        "        responses to stimuli with similar angles falling within specified bins.\n",
        "    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation\n",
        "        of each stimulus, in degrees. This is actually the mean orientation\n",
        "        of all stimuli in each bin.\n",
        "\n",
        "  \"\"\"\n",
        "  with np.load(data_name) as dobj:\n",
        "    data = dict(**dobj)\n",
        "  resp = data['resp']\n",
        "  stimuli = data['stimuli']\n",
        "\n",
        "  if bin_width > 1:\n",
        "    # Bin neural responses and stimuli\n",
        "    bins = np.digitize(stimuli, np.arange(0, 360 + bin_width, bin_width))\n",
        "    stimuli_binned = np.array([stimuli[bins == i].mean() for i in np.unique(bins)])\n",
        "    resp_binned = np.array([resp[bins == i, :].mean(0) for i in np.unique(bins)])\n",
        "  else:\n",
        "    resp_binned = resp\n",
        "    stimuli_binned = stimuli\n",
        "\n",
        "  # Return as torch.Tensor\n",
        "  resp_tensor = torch.tensor(resp_binned, dtype=torch.float32)\n",
        "  stimuli_tensor = torch.tensor(stimuli_binned, dtype=torch.float32).unsqueeze(1)  # add singleton dimension to make a column vector\n",
        "\n",
        "  return resp_tensor, stimuli_tensor\n",
        "\n",
        "\n",
        "def plot_data_matrix(X, ax):\n",
        "  \"\"\"Visualize data matrix of neural responses using a heatmap\n",
        "\n",
        "  Args:\n",
        "    X (torch.Tensor or np.ndarray): matrix of neural responses to visualize\n",
        "        with a heatmap\n",
        "    ax (matplotlib axes): where to plot\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  cax = ax.imshow(X, cmap='viridis', vmin=np.percentile(X, 1), vmax=np.percentile(X, 99))\n",
        "  cbar = plt.colorbar(cax, ax=ax, label='normalized neural response')\n",
        "\n",
        "  ax.set_aspect('auto')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "\n",
        "def identityLine():\n",
        "  \"\"\"\n",
        "  Plot the identity line y=x\n",
        "  \"\"\"\n",
        "  ax = plt.gca()\n",
        "  lims = np.array([ax.get_xlim(), ax.get_ylim()])\n",
        "  minval = lims[:, 0].min()\n",
        "  maxval = lims[:, 1].max()\n",
        "  equal_lims = [minval, maxval]\n",
        "  ax.set_xlim(equal_lims)\n",
        "  ax.set_ylim(equal_lims)\n",
        "  line = ax.plot([minval, maxval], [minval, maxval], color=\"0.7\")\n",
        "  line[0].set_zorder(-1)\n",
        "\n",
        "def get_data(n_stim, train_data, train_labels):\n",
        "  \"\"\" Return n_stim randomly drawn stimuli/resp pairs\n",
        "\n",
        "  Args:\n",
        "    n_stim (scalar): number of stimuli to draw\n",
        "    resp (torch.Tensor):\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "\n",
        "  Returns:\n",
        "    (torch.Tensor, torch.Tensor): n_stim x n_neurons tensor of neural responses and n_stim x 1 of orientations respectively\n",
        "  \"\"\"\n",
        "  n_stimuli = train_labels.shape[0]\n",
        "  istim = np.random.choice(n_stimuli, n_stim)\n",
        "  r = train_data[istim]  # neural responses to this stimulus\n",
        "  ori = train_labels[istim]  # true stimulus orientation\n",
        "\n",
        "  return r, ori\n",
        "\n",
        "def stimulus_class(ori, n_classes):\n",
        "  \"\"\"Get stimulus class from stimulus orientation\n",
        "\n",
        "  Args:\n",
        "    ori (torch.Tensor): orientations of stimuli to return classes for\n",
        "    n_classes (int): total number of classes\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: 1D tensor with the classes for each stimulus\n",
        "\n",
        "  \"\"\"\n",
        "  bins = np.linspace(0, 360, n_classes + 1)\n",
        "  return torch.tensor(np.digitize(ori.squeeze(), bins)) - 1  # minus 1 to accomodate Python indexing\n",
        "\n",
        "def plot_decoded_results(train_loss, test_labels, predicted_test_labels):\n",
        "  \"\"\" Plot decoding results in the form of network training loss and test predictions\n",
        "\n",
        "  Args:\n",
        "    train_loss (list): training error over iterations\n",
        "    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    predicted_test_labels (torch.Tensor): n_test x 1 tensor with predicted orientations of the\n",
        "      stimuli from decoding neural network\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Plot results\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "  # Plot the training loss over iterations of GD\n",
        "  ax1.plot(train_loss)\n",
        "\n",
        "  # Plot true stimulus orientation vs. predicted class\n",
        "  ax2.plot(stimuli_test.squeeze(), predicted_test_labels, '.')\n",
        "\n",
        "  ax1.set_xlim([0, None])\n",
        "  ax1.set_ylim([0, None])\n",
        "  ax1.set_xlabel('iterations of gradient descent')\n",
        "  ax1.set_ylabel('negative log likelihood')\n",
        "  ax2.set_xlabel('true stimulus orientation ($^o$)')\n",
        "  ax2.set_ylabel('decoded orientation bin')\n",
        "  ax2.set_xticks(np.linspace(0, 360, n_classes + 1))\n",
        "  ax2.set_yticks(np.arange(n_classes))\n",
        "  class_bins = [f'{i * 360 / n_classes: .0f}$^o$ - {(i + 1) * 360 / n_classes: .0f}$^o$' for i in range(n_classes)]\n",
        "  ax2.set_yticklabels(class_bins);\n",
        "\n",
        "  # Draw bin edges as vertical lines\n",
        "  ax2.set_ylim(ax2.get_ylim())  # fix y-axis limits\n",
        "  for i in range(n_classes):\n",
        "    lower = i * 360 / n_classes\n",
        "    upper = (i + 1) * 360 / n_classes\n",
        "    ax2.plot([lower, lower], ax2.get_ylim(), '-', color=\"0.7\", linewidth=1, zorder=-1)\n",
        "    ax2.plot([upper, upper], ax2.get_ylim(), '-', color=\"0.7\", linewidth=1, zorder=-1)\n",
        "\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHydAad-_w35"
      },
      "source": [
        "###What kind of data are we working with?\n",
        "\n",
        "This data is from Carsen Stringer's paper on BioRxiv, linked [here](https://www.biorxiv.org/content/10.1101/679324v2). Headfixed mice were able to run in place on an air-floating ball while static gratings (1-360&deg;, 1&deg; gradations) were presented to the L eye for 750ms in a random order. While not an incredibly interesting setup, they were able to obtain a lot of data.\n",
        "\n",
        "Neural activity was measured with multi-plane calcium imaging, and over 20k neurons were able to be identified. Activity in each neuron was binned and averaged across the presentation of each stimulus. \n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.imgur.com/TsuJdAh.png\" width=\"850\" />\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGVc9DWH_aCN",
        "cellView": "form"
      },
      "source": [
        "#@title Data summary\n",
        "\n",
        "# Load data\n",
        "resp_all, stimuli_all = load_data()  # argument to this function specifies bin width\n",
        "n_stimuli, n_neurons = resp_all.shape\n",
        "\n",
        "print(f'{n_neurons} neurons in response to {n_stimuli} stimuli')\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(2 * 9, 7.5))\n",
        "\n",
        "# Visualize data matrix\n",
        "plot_data_matrix(resp_all[:100, :].T, ax1)  # plot responses of first 100 neurons\n",
        "ax1.set_xlabel('stimulus')\n",
        "ax1.set_ylabel('neuron')\n",
        "\n",
        "# Plot tuning curves of three random neurons\n",
        "ineurons = np.random.choice(n_neurons, 3, replace=False)  # pick three random neurons\n",
        "ax2.plot(stimuli_all, resp_all[:, ineurons])\n",
        "ax2.set_xlabel('stimulus orientation ($^o$)')\n",
        "ax2.set_ylabel('neural response')\n",
        "ax2.set_xticks(np.linspace(0, 360, 5))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3TPeWF7BqKU"
      },
      "source": [
        "## Intro to *(decoding)* neural networks\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/static/one-layer-network.png?raw=true\" width=\"450\" />\n",
        "</p>\n",
        "\n",
        "Above is a generalized model of how a network with one so-called *hidden layer* might be organized for this dataset. Each real neuron's response to a particular stimulus is z-scored and attributed to one \"neuron\" in the input layer, here denoted as **r**. Each input neuron is connected to every neuron in the next layer, with connectivity specified by the weight matrix **w**; the activity in **r** is multiplied by its corresponding weight **w** and then fed into an activation function, two examples of which are below: sigmoid and ReLU. These generally non-linear functions serve to make the network more flexible (I'm not sure exactly what this means tbh). ReLU is viewed as a more biologically plausible mechanism as its activity is contstrained to the range [0,inf) and doesn't saturate at high inputs.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/max/1452/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"550\" />\n",
        "</p>\n",
        "\n",
        "Below is the total computation that is performed going from the input layer to the hidden layer. Bias terms can be added here to control for baseline neural activity.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.imgur.com/PvKJvHC.png\" width=\"550\" />\n",
        "</p>\n",
        "\n",
        "Generally, when starting a new neural network, all of the weights and biases are totally random. We can set this up using PyTorch below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztpm2M-xFIWp"
      },
      "source": [
        "# Our first network (with no learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or012TulG1z7",
        "cellView": "form"
      },
      "source": [
        "#@title Execute this cell to split data into training and test sets\n",
        "#@markdown We'll use this to assess the accuracy of our network.\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(4)\n",
        "torch.manual_seed(4)\n",
        "\n",
        "# Split data into training set and testing set\n",
        "n_train = int(0.6 * n_stimuli)  # use 60% of all data for training set\n",
        "ishuffle = torch.randperm(n_stimuli)\n",
        "itrain = ishuffle[:n_train]  # indices of data samples to include in training set\n",
        "itest = ishuffle[n_train:]  # indices of data samples to include in testing set\n",
        "stimuli_test = stimuli_all[itest]\n",
        "resp_test = resp_all[itest]\n",
        "stimuli_train = stimuli_all[itrain]\n",
        "resp_train = resp_all[itrain]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QelkqnoMFibi"
      },
      "source": [
        "## Set up the class for the network\n",
        "\n",
        "The exact syntax of this PyTorch instantiation is obviously not the point here. But, we can gain some simple intution about what it's doing regardless of how much Python we know. We'll also be able to see just how easy these are to set up.\n",
        "\n",
        "**def init**: set up the number of layers and units in each layer; here, we have an imput layer that has the same number of neurons as were recorded in the dataset which is connected to a hidden layer. The hidden layer then has a single output, which will be the decoded orientation.\n",
        "\n",
        "**def forward**: this describes the relationship between the layers (i.e. how is data passed *forward* through the network); in this simple example, all this says is that the input layer accepts neural data and outputs to the hidden layer, then that the output y is the output of the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuSC5hcTFXf1",
        "cellView": "both"
      },
      "source": [
        "class DeepNet(nn.Module):\n",
        "  \"\"\"Deep Network with one hidden layer\n",
        "\n",
        "  Args:\n",
        "    n_inputs (int): number of input units\n",
        "    n_hidden (int): number of units in hidden layer\n",
        "\n",
        "  Attributes:\n",
        "    in_layer (nn.Linear): weights and biases of input layer\n",
        "    out_layer (nn.Linear): weights and biases of output layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "    \"\"\"Decode stimulus orientation from neural responses\n",
        "\n",
        "    Args:\n",
        "      r (torch.Tensor): vector of neural responses to decode, must be of\n",
        "        length n_inputs. Can also be a tensor of shape n_stimuli x n_inputs,\n",
        "        containing n_stimuli vectors of neural responses\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: network outputs for each input provided in r. If\n",
        "        r is a vector, then y is a 1D tensor of length 1. If r is a 2D\n",
        "        tensor then y is a 2D tensor of shape n_stimuli x 1.\n",
        "\n",
        "    \"\"\"\n",
        "    h = self.in_layer(r)  # the input layer takes in r (neural responses) and outputs h (hidden representation)\n",
        "    y = self.out_layer(h) # the output layer takes in h and outputs y (decoded stimulus direction)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmq6gGn8Fy9y"
      },
      "source": [
        "## Initialize the network\n",
        "\n",
        "Here, with no training, we'll implement our network. The input to the network is the z-scored responses to one stimulus for each of the 20k neurons, and the output is the decoded stimulus orientation. It will be very clear that our network is useless at this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKC2GYQPFfI0"
      },
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize a deep network with M=200 hidden units\n",
        "net = DeepNet(n_neurons, 200)\n",
        "\n",
        "# Get neural responses (r) to and orientation (ori) to one stimulus in dataset\n",
        "r, ori = get_data(1, resp_train, stimuli_train)  # using helper function get_data\n",
        "\n",
        "# Decode orientation from these neural responses using initialized network\n",
        "out = net(r)  # compute output from network, equivalent to net.forward(r)\n",
        "\n",
        "print('decoded orientation: %.2f degrees' % out)\n",
        "print('true orientation: %.2f degrees' % ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sky-5IzOHXbe"
      },
      "source": [
        "## Let's add an activation function (here ReLU)\n",
        "\n",
        "Spoiler: the model is still useless because it hasn't been trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JplXNmBzHkED"
      },
      "source": [
        "class DeepNetReLU(nn.Module):\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "\n",
        "    h = torch.relu(self.in_layer(r)) # This is the only change from the previous version\n",
        "    y = self.out_layer(h)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Get neural responses (r) to and orientation (ori) to one stimulus in dataset\n",
        "r, ori = get_data(1, resp_train, stimuli_train)\n",
        "\n",
        "\n",
        "# Initialize deep network with M=20 hidden units\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Decode orientation from these neural responses using initialized network\n",
        "# net(r) is equivalent to net.forward(r)\n",
        "out = net(r)\n",
        "\n",
        "print('decoded orientation: %.2f degrees' % out)\n",
        "print('true orientation: %.2f degrees' % ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB2pzZjjH8-d"
      },
      "source": [
        "# Let's train our network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai-2TeNtICNK"
      },
      "source": [
        "*This section is copied directly from NMA as I liked the way they explained it*\n",
        "\n",
        "Because the weights of the network are currently randomly chosen, the outputs of the network are nonsense: the decoded stimulus orientation is nowhere close to the true stimulus orientation. We'll shortly write some code to change these weights so that the network does a better job of decoding.\n",
        "\n",
        "But to do so, we first need to define what we mean by \"better\". One simple way of defining this is to use the squared error\n",
        "\\begin{equation}\n",
        "    L = (y - \\tilde{y})^2\n",
        "\\end{equation}\n",
        "where $y$ is the network output and $\\tilde{y}$ is the true stimulus orientation. When the decoded stimulus orientation is far from the true stimulus orientation, $L$ will be large. We thus refer to $L$ as the **loss function**, as it quantifies how *bad* the network is at decoding stimulus orientation.\n",
        "\n",
        "PyTorch actually carries with it a number of built-in loss functions. The one corresponding to the squared error is called `nn.MSELoss()`. This will take as arguments a **batch** of network outputs $y_1, y_2, \\ldots, y_P$ and corresponding target outputs $\\tilde{y}_1, \\tilde{y}_2, \\ldots, \\tilde{y}_P$, and compute the **mean squared error (MSE)**\n",
        "\\begin{equation}\n",
        "    L = \\frac{1}{P}\\sum_{n=1}^P \\left(y^{(n)} - \\tilde{y}^{(n)}\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "*I wrote this next part*\n",
        "\n",
        "It's important to note that the loss function we're using here is the **mean** squared error. By optimizing the network this way, we're ensuring that the network can accurately predict values across the whole range of stimulus orientations.\n",
        "\n",
        "The code below calculates the MSE for our untrained network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKa9FuuhIl3c"
      },
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize a deep network with M=20 hidden units\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Get neural responses to first 20 stimuli in the data set\n",
        "r, ori = get_data(20, resp_train, stimuli_train)\n",
        "\n",
        "# Decode orientation from these neural responses\n",
        "out = net(r)\n",
        "\n",
        "# Initialize PyTorch mean squared error loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Evaluate mean squared error\n",
        "loss = loss_fn(out, ori)\n",
        "\n",
        "print('mean squared error: %.2f' % loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ZA9bMSI5uf"
      },
      "source": [
        "## Let's use stochastic gradient descent optimize the network by minimizing our MSE\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=\"550\" />\n",
        "</p>\n",
        "\n",
        "Here's how this works. For a given weight **w**, the cost associated with that weight can be computed (for us here, this is the mean squared error value). Using some fun linear algebra magic, the gradient can be computed (this is essentially the derivative at this point). Based on the sign of the slope of the gradient, the algorithm can determine which direction approaches the nearest local minimum; then, the algorithm makes incremental steps toward that minimum based on some learning rate (so each step isnt too big).\n",
        "\n",
        "To optimize the whole network, PyTorch does this for every single weight, and averages the suggested steps across every training example. You can easily imagine that this becomes quite computationally intense when there are many thousands of training examples, which is where the stochastic in SGD comes in. Instead of doing this for every training example, PyTorch will randomly choose a subset of training examples and run SGD only on those; after multiple iterations, you generally arrive where you should - even if it looks a bit zig-zaggy.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.imgur.com/8GhLbE4.png\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "This process of optimization is often referred to as backpropagation, as the model first optimizes the weights between the final two layers, then moves \"backwards\" to optimize the weights in the preceding layers. Information also is thought to flow backwards since you are starting from your cost function (MSE), which is of course based on the outputs, and working through the network in the opposite direction that information usually flows.\n",
        "\n",
        "Below, see how we can train our network using a SGD optimizer. Again, the logic of the PyTorch notation is not important here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xom2RvnEI-_w",
        "cellView": "form"
      },
      "source": [
        "#@title Run SGD training\n",
        "\n",
        "def train(net, loss_fn, train_data, train_labels, n_iter=50, learning_rate=1e-4):\n",
        "  \"\"\"Run gradient descent to opimize parameters of a given network\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): PyTorch network whose parameters to optimize\n",
        "    loss_fn: built-in PyTorch loss function to minimize\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    n_iter (int): number of iterations of gradient descent to run\n",
        "    learning_rate (float): learning rate to use for gradient descent\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize PyTorch SGD optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Placeholder to save the loss at each iteration\n",
        "  track_loss = []\n",
        "\n",
        "  # Loop over epochs (cf. appendix)\n",
        "  for i in range(n_iter):\n",
        "\n",
        "    # Evaluate loss using loss_fn\n",
        "    out = net(train_data)  # compute network output from inputs in train_data\n",
        "    loss = loss_fn(out, train_labels)  # evaluate loss function\n",
        "\n",
        "    # Compute gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store current value of loss\n",
        "    track_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_iter // 5) == 0:\n",
        "      print(f'iteration {i + 1}/{n_iter} | loss: {loss.item():.3f}')\n",
        "\n",
        "  return track_loss\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize network\n",
        "net = DeepNetReLU(n_neurons, 20)\n",
        "\n",
        "# Initialize built-in PyTorch MSE loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Run GD on data\n",
        "train_loss = train(net, loss_fn, resp_train, stimuli_train)\n",
        "\n",
        "# Plot the training loss over iterations of GD\n",
        "plt.plot(train_loss)\n",
        "plt.xlim([0, None])\n",
        "plt.ylim([0, None])\n",
        "plt.xlabel('iterations of gradient descent')\n",
        "plt.ylabel('mean squared error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfJJp7IlNOak",
        "cellView": "form"
      },
      "source": [
        "#@title Let's see how we did\n",
        "#@markdown Execute this cell to evaluate and plot test error\n",
        "\n",
        "out = net(resp_test)  # decode stimulus orientation for neural responses in testing set\n",
        "ori = stimuli_test  # true stimulus orientations\n",
        "test_loss = loss_fn(out, ori)  # MSE on testing set (Hint: use loss_fn initialized in previous exercise)\n",
        "\n",
        "plt.plot(ori, out.detach(), '.')  # N.B. need to use .detach() to pass network output into plt.plot()\n",
        "identityLine()  # draw the identity line y=x; deviations from this indicate bad decoding!\n",
        "plt.title('MSE on testing set: %.2f' % test_loss.item())  # N.B. need to use .item() to turn test_loss into a scalar\n",
        "plt.xlabel('true stimulus orientation ($^o$)')\n",
        "plt.ylabel('decoded stimulus orientation ($^o$)')\n",
        "axticks = np.linspace(0, 360, 5)\n",
        "plt.xticks(axticks)\n",
        "plt.yticks(axticks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACSXJxxjNTs5"
      },
      "source": [
        "We can see that besides having some issues around the 0 and 360 points, our model seems to predict with reasonable accuracy the orientation of the visual stimulus using the activity of over 20k neurons. If you feel like it, you could try messing around with the numbers of neurons in each layer in the code above to see how performance changes - but we can see that it does pretty well already.\n",
        "\n",
        "Using a more complex loss function that knows that 0&deg; and 360&deg; are the same and that 1&deg; and 359&deg; are very close to each other, it would not be too tricky to make this network's performance even better.\n",
        "\n",
        "This method of optimization is referred to as **supervised learning**, as we provided the network with both the inputs and the correct outputs. \n",
        "\n",
        "One could imagine a network that is trained under a **reinforcement learning** algorithm, where the choices/decisions determine the amount of \"reward\" at the end of each trial; the network thus aims to maximize the amount of reward.\n",
        "\n",
        "The third and final kind of learning algorithm is **unsupervised learning**, where the network is provided with only the inputs and then is allowed to \"do whatever it wants,\" without an objective or loss function to guide its parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_abXCJdunH6"
      },
      "source": [
        "# Variations on this theme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKwg3LFyV58D"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/cb8aed99-35a4-41dd-9d40-b7925ad49394/gr2.jpg\" width=\"800\" />\n",
        "</p>\n",
        "\n",
        "This figure is from [this](https://doi.org/10.1016/j.neuron.2020.09.005) primer; here are shown 4 different kinds of neural networks that are often used for different purposes.\n",
        "\n",
        "In **A** is the simplest kind of network, which looks a lot like what we just built. Some series of inputs is processed by a number of layers, eventually resulting in some kind of output. Some people like to call this a \"multi-layer perceptron.\" These have been around for a long time now. They are often viewed as a *black box*, as there is also no time component here; so inputs instantly become outputs after the network has been trained.\n",
        "\n",
        "**B** is an example of a recurrent neural network. The general principle is the same: give it data, it does some fancy linear algebra, and spits out different data. Of course the architecture or structure of how the units are connected to one another is vastly different than what we were just working with. The connections in this network don't have to be one-to-one nor as \"linear\" in direction as the network in A. These are the kinds of networks that Kanaka's group builds and studies (an example of which is shown below, from their recent [review](https://doi-org.eresources.mssm.edu/10.1016/j.conb.2020.11.003)). Importantly, these networks also process information in time, so they can be analysed more similarly to brains (activity of \"units\", etc.) than perceptrons.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.imgur.com/x5va3ll.jpg\"width=\"600\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "**C** dives a bit into the math that the model in B might be doing. Each gray box is a different point in time, and *c* and *r* represent the current state and corresponding output of the network at each time *t*. *x* is the input to the system that drives the change between states.\n",
        "\n",
        "**D** illustrates what a convolutional neural network does. To try to simplify how it works, I like to think of it like the visual system; this analogy is especially relevant since these networks are often used for computer vision and image classification. Each \"neuron\" in a channel has a particular \"receptive field\" called a kernel. The dimensions of this kernel determine the size of the \"receptive field\" for each unit. The kernel then slides along the image being processed in a moving window fashion. The weights of the network are shared amongst all of the units and tell each neuron which parts of its \"receptive field\" to care about. Below is an example of a simple 2D gaussian, which would tell the network that the center of the \"receptive field\" should be weighted more highly than the outside.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"http://campar.in.tum.de/twiki/pub/Chair/HaukeHeibelGaussianDerivatives/gauss2d00.png\"width=\"400\" />\n",
        "</p>\n",
        "\n",
        "Each channel or layer of the network will have different kernel sizes and weight functions to detect different features of the image being processed. Put together, all of these convolutional layers generally feed into some kind of pooling layer, which then is connected to an output layer. Below is an example from Neuromatch Academy of how this might work (without the pooling layer). Note that this network goes in the opposite direction of the one we built above, as it takes the stimulus presented as an imput and tries to predict the neural activity.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/static/conv_fc.PNG?raw=true\" width=\"800\" />\n",
        "</p>\n",
        "\n",
        "In a convolutional neural network trained to identify handwritten digits from the MNIST dataset (which machine learning folks love), one can examine the responses of layers in the network (A: 2 convolutional layers, 2 fully connected layers, 1 output layer). \n",
        "\n",
        "B shows responses of neurons to each of the 10 digits in 2 of the layers.\n",
        "\n",
        "C shows individual neurons' \"receptive fields.\" Layers 1 & 2 show spatial locations as they're convolutional layers, while 3 & 4 are not convolutional and this show no spatial organization.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0896627320307054-gr4.jpg\"width=\"600\" />\n",
        "</p>\n",
        "\n",
        "(Image from [this](https://www.sciencedirect.com/science/article/pii/S0896627320307054) primer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5Au39UrmK0Y"
      },
      "source": [
        "It's a more complicated version of a ConvNet like this that underlies [DeepLabCut](https://www.nature.com/articles/s41593-018-0209-y) (shown below). ResNet-50 is a specific network architecture (containing 50 layers), and ImageNet is a standardized training set with nearly 15 million labelled images. Then, the deconvolution layers replace the detected points (arms, legs, head, etc.) back on the original video to allow tracking.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.imgur.com/i0e7ecf.png\"width=\"400\" />\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U3vcfPPuv0s"
      },
      "source": [
        "# More info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv5xNyipixAz"
      },
      "source": [
        "This was a \"quick and dirty\" intro to the land of neural networks. If you are interested in more reading (or video watching), here are some links to resources I used when putting this together.\n",
        "\n",
        "Neuromatch Academy had two full days of neural network and deep learning content, complete with videos and Colab notebook tutorials. Day 1 and Day 2 notebooks are [here](https://github.com/NeuromatchAcademy/course-content/tree/master/tutorials/W3D4_DeepLearning1) and [here](https://github.com/NeuromatchAcademy/course-content/tree/master/tutorials/W3D5_DeepLearning2), and YouTube playlists for the accompanying videos are [here](https://www.youtube.com/playlist?list=PLkBQOLLbi18PZ2uw0p7G4EkzjzqP8l0Eg) and [here](https://www.youtube.com/playlist?list=PLkBQOLLbi18Ojl1CV8W00JZ0C0hivBxw1).\n",
        "\n",
        "[Link](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi): 3Blue1Brown's YouTube playlist of neural network videos. 3 videos about 20 min each. A bit more math than necessary but well-explained.\n",
        "\n",
        "[Link](https://www.sciencedirect.com/science/article/pii/S0896627320307054): *Artificial Neural Networks for Neuroscientists: A Primer*. They also have a [GitHub repo](https://github.com/gyyang/nn-brain) with tutorial notebooks, but they aren't super well annotated to help understand what's going on.\n",
        "\n",
        "A lot of online resources are not appropriate for our understanding of ANNs. Either they are too deep in the math that it is impossible to understand the general principles, or too deep in Python syntax to intuitively understand what's happening. Resources by machine learning or AI people tend to tout these as magic, when in reality they're quite simple :)"
      ]
    }
  ]
}